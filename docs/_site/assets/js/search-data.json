{"0": {
    "doc": "DAME",
    "title": "dame_flame.matching.DAME",
    "content": "The DAME algorithm class . class dame_flame.matching.DAME(adaptive_weights='ridge', alpha=0.1, repeats=True, verbose=2, early_stop_iterations=False, stop_unmatched_c=False, early_stop_un_c_frac=False, stop_unmatched_t=False, early_stop_un_t_frac=False, early_stop_pe=False, early_stop_pe_frac=0.01, early_stop_bf=False, early_stop_bf_frac=0.01, missing_indicator=np.nan, missing_data_replace=0, missing_holdout_replace=0, missing_holdout_imputations=10, missing_data_imputations=1, want_pe=False, want_bf=False) . ",
    "url": "http://localhost:4000/documentation/api-documentation/DAME#dame_flamematchingdame",
    "relUrl": "/documentation/api-documentation/DAME#dame_flamematchingdame"
  },"1": {
    "doc": "DAME",
    "title": "Parameters",
    "content": "adaptive_weights: bool, “ridge”, “decision tree”, “ridgeCV”, optional (default=”ridge”) The method used to decide what covariate set should be dropped next. alpha: float, optional (default=0.1) If adaptive_weights is set to ridge, this is the alpha for ridge regression. repeats: Bool, optional (default=True) Whether or not units for whom a main matched has been found can be used again, and placed in an auxiliary matched group. verbose: int 0,1,2,3 (default=2) Style of printout while algorithm runs. If 0, no output If 1, provides iteration number If 2, provides iteration number and additional information on the progress of the matching at every 10th iteration If 3, provides iteration number and additional information on the progress of the matching at every iteration . early_stop_iterations: int, optional (default=0) If provided, a number of iterations after which to hard stop the algorithm. stop_unmatched_c: bool, optional (default=False) If True, then the algorithm terminates when there are no more control units to match. stop_unmatched_t: bool, optional (default=False) If True, then the algorithm terminates when there are no more treatment units to match. early_stop_un_c_frac: float from 0.0 to 1.0, optional (default=0.1) This provides a fraction of unmatched control units. When the threshold is met, the algorithm will stop iterating. For example, using an input dataset with 100 control units, the algorithm will stop when 10 control units are unmatched and 90 are matched (or earlier, depending on other stopping conditions). early_stop_un_t_frac: float from 0.0 to 1.0, optional (default=0.1) This provides a fraction of unmatched treatment units. When the threshold is met, the algorithm will stop iterating. For example, using an input dataset with 100 treatment units, the algorithm will stop when 10 control units are unmatched and 90 are matched (or earlier, depending on other stopping conditions). early_stop_pe: bool, optional (default=False) If this is true, then if the covariate set chosen for matching has a predictive error higher than the parameter early_stop_pe_frac, the algorithm will stop. early_stop_pe_frac: float, optional (default=0.01) If early_stop_pe is true, then if the covariate set chosen for matching has a predictive error higher than this value, the algorithm will stop. early_stop_bf: bool, optional (default=False) If this is true, then if the covariate set chosen for matching has a balancing factor lower than early_stop_bf_frac, then the algorithm will stop. early_stop_bf_frac: float, optional (default=0.01) If early_stop_bf is true, then if the covariate set chosen for matching has a balancing factor lower than this value, then the algorithm will stop. want_pe: bool, optional (default=False) If true, the output of the algorithm will include the predictive error of the covariate sets used for matching in each iteration. want_bf: bool, optional (default=False) If true, the output will include the balancing factor for each iteration. missing_indicator: character, integer, numpy.nan, optional (default=numpy.nan) This is the indicator for missing data in the dataset. missing_holdout_replace: int 0,1,2, optional (default=0) If 0, assume no missing holdout data and proceed. If 1, the algorithm excludes units with missing values from the holdout dataset. If 2, do MICE on holdout dataset. If this option is selected, it will be done for a number of iterations equal to missing_holdout_imputations. missing_data_replace: int 0,1,2,3, optional, (default=0) If 0, assume no missing data in matching data and proceed. If 1, the algorithm does not match on units that have missing values. If 2, prevent all missing_indicator values from being matched on. If 3, do MICE on matching dataset. This is not recommended. If this option is selected, it will be done for a number of iterations equal to missing_data_imputations. missing_holdout_imputations: int, optional (default=10) If missing_holdout_replace=2, the number of imputations. missing_data_imputations: int, optional (default=1) If missing_data_replace=3, the number of imputations. ",
    "url": "http://localhost:4000/documentation/api-documentation/DAME#parameters",
    "relUrl": "/documentation/api-documentation/DAME#parameters"
  },"2": {
    "doc": "DAME",
    "title": "References",
    "content": "Dieng, Awa, et al. “Interpretable almost-exact matching for causal inference.” . ",
    "url": "http://localhost:4000/documentation/api-documentation/DAME#references",
    "relUrl": "/documentation/api-documentation/DAME#references"
  },"3": {
    "doc": "DAME",
    "title": "Example",
    "content": "import pandas as pd import dame_flame df = pd.read_csv(\"dame_flame/data/sample.csv\") model = dame_flame.matching.DAME(repeats=False, verbose=1, early_stop_iterations=False) model.fit(holdout_data=df) result = model.predict(input_data=df) print(result) #&gt; x1 x2 x3 x4 #&gt; 0 0 1 1 * #&gt; 1 0 1 1 * #&gt; 2 1 0 * 1 #&gt; 3 1 0 * 1 . ",
    "url": "http://localhost:4000/documentation/api-documentation/DAME#example",
    "relUrl": "/documentation/api-documentation/DAME#example"
  },"4": {
    "doc": "DAME",
    "title": "Methods",
    "content": "| init(self, adaptive_weights, alpha,…) | Initialize self | . | fit(self, holdout_data, treatment_col….) | Provide self with holdout data | . | predict(self, input_data…) | Perform the match on the input data | . init(adaptive_weights=’ridge’, alpha=0.1, repeats=True, verbose=2, early_stop_iterations=False, stop_unmatched_c=False, early_stop_un_c_frac=False, stop_unmatched_t=False, early_stop_un_t_frac=False, early_stop_pe=False, early_stop_pe_frac=0.01, early_stop_bf=False, early_stop_bf_frac=0.01, missing_indicator=np.nan, missing_data_replace=0, missing_holdout_replace=0, missing_holdout_imputations=10, missing_data_imputations=1, want_pe=False, want_bf=False) . Initialize self . fit(self, holdout_data=False, treatment_column_name=’treated’, outcome_column_name=’outcome’, weight_array=False)) . Provide self with holdout data . Parameters . treatment_column_name: string, optional (default=”treated”) This is the name of the column with a binary indicator for whether a row is a treatment or control unit. outcome_column_name: string, optional (default=”outcome”) This is the name of the column with the outcome variable of each unit. adaptive_weights: bool, “ridge”, “decision tree”, “ridgeCV”, optional (default=”ridge”) The method used to decide what covariate set should be dropped next. weight_array: array, optional If adaptive_weights = False, these are the weights to the covariates in input_data, for the non-adaptive version of DAME. Must sum to 1. In this case, we do not use machine learning for the weights, they are manually entered as weight_array. predict(self, input_data) . Perform match and return matches . | Parameters: | input_data: dataframe-like (default=None). The dataframe on which to perform the matching | . | Returns: | Result: Pandas dataframe of matched units and covariates matched on | . ",
    "url": "http://localhost:4000/documentation/api-documentation/DAME#methods",
    "relUrl": "/documentation/api-documentation/DAME#methods"
  },"5": {
    "doc": "DAME",
    "title": "DAME",
    "content": " ",
    "url": "http://localhost:4000/documentation/api-documentation/DAME",
    "relUrl": "/documentation/api-documentation/DAME"
  },"6": {
    "doc": "FLAME",
    "title": "dame_flame.matching.FLAME",
    "content": "The FLAME algorithm class . class dame_flame.matching.FLAME(adaptive_weights='ridge', alpha=0.1, repeats=True, verbose=2, early_stop_iterations=False, stop_unmatched_c=False, early_stop_un_c_frac=False, stop_unmatched_t=False, early_stop_un_t_frac=False, early_stop_pe=False, early_stop_pe_frac=0.01, early_stop_bf=False, early_stop_bf_frac=0.01, missing_indicator=np.nan, missing_data_replace=0, missing_holdout_replace=0, missing_holdout_imputations=10, missing_data_imputations=1, want_pe=False, want_bf=False) . ",
    "url": "http://localhost:4000/documentation/api-documentation/FLAME#dame_flamematchingflame",
    "relUrl": "/documentation/api-documentation/FLAME#dame_flamematchingflame"
  },"7": {
    "doc": "FLAME",
    "title": "Parameters",
    "content": "adaptive_weights: bool, “ridge”, “decision tree”, “ridgeCV”, optional (default=”ridge”) The method used to decide what covariate set should be dropped next. alpha: float, optional (default=0.1) If adaptive_weights is set to ridge, this is the alpha for ridge regression. repeats: Bool, optional (default=True) Whether or not units for whom a main matched has been found can be used again, and placed in an auxiliary matched group. verbose: int 0,1,2,3 (default=2) Style of printout while algorithm runs. If 0, no output If 1, provides iteration number If 2, provides iteration number and additional information on the progress of the matching at every 10th iteration If 3, provides iteration number and additional information on the progress of the matching at every iteration . early_stop_iterations: int, optional (default=0) If provided, a number of iterations after which to hard stop the algorithm. stop_unmatched_c: bool, optional (default=False) If True, then the algorithm terminates when there are no more control units to match. stop_unmatched_t: bool, optional (default=False) If True, then the algorithm terminates when there are no more treatment units to match. early_stop_un_c_frac: float from 0.0 to 1.0, optional (default=0.1) This provides a fraction of unmatched control units. When the threshold is met, the algorithm will stop iterating. For example, using an input dataset with 100 control units, the algorithm will stop when 10 control units are unmatched and 90 are matched (or earlier, depending on other stopping conditions). early_stop_un_t_frac: float from 0.0 to 1.0, optional (default=0.1) This provides a fraction of unmatched treatment units. When the threshold is met, the algorithm will stop iterating. For example, using an input dataset with 100 treatment units, the algorithm will stop when 10 control units are unmatched and 90 are matched (or earlier, depending on other stopping conditions). early_stop_pe: bool, optional (default=False) If this is true, then if the covariate set chosen for matching has a predictive error higher than the parameter early_stop_pe_frac, the algorithm will stop. early_stop_pe_frac: float, optional (default=0.01) If early_stop_pe is true, then if the covariate set chosen for matching has a predictive error higher than this value, the algorithm will stop. early_stop_bf: bool, optional (default=False) If this is true, then if the covariate set chosen for matching has a balancing factor lower than early_stop_bf_frac, then the algorithm will stop. early_stop_bf_frac: float, optional (default=0.01) If early_stop_bf is true, then if the covariate set chosen for matching has a balancing factor lower than this value, then the algorithm will stop. want_pe: bool, optional (default=False) If true, the output of the algorithm will include the predictive error of the covariate sets used for matching in each iteration. want_bf: bool, optional (default=False) If true, the output will include the balancing factor for each iteration. missing_indicator: character, integer, numpy.nan, optional (default=numpy.nan) This is the indicator for missing data in the dataset. missing_holdout_replace: int 0,1,2, optional (default=0) If 0, assume no missing holdout data and proceed. If 1, the algorithm excludes units with missing values from the holdout dataset. If 2, do MICE on holdout dataset. If this option is selected, it will be done for a number of iterations equal to missing_holdout_imputations. missing_data_replace: int 0,1,2,3, optional, (default=0) If 0, assume no missing data in matching data and proceed. If 1, the algorithm does not match on units that have missing values. If 2, prevent all missing_indicator values from being matched on. If 3, do MICE on matching dataset. This is not recommended. If this option is selected, it will be done for a number of iterations equal to missing_data_imputations. missing_holdout_imputations: int, optional (default=10) If missing_holdout_replace=2, the number of imputations. missing_data_imputations: int, optional (default=1) If missing_data_replace=3, the number of imputations. ",
    "url": "http://localhost:4000/documentation/api-documentation/FLAME#parameters",
    "relUrl": "/documentation/api-documentation/FLAME#parameters"
  },"8": {
    "doc": "FLAME",
    "title": "References",
    "content": "Dieng, Awa, et al. “Interpretable almost-exact matching for causal inference.” . ",
    "url": "http://localhost:4000/documentation/api-documentation/FLAME#references",
    "relUrl": "/documentation/api-documentation/FLAME#references"
  },"9": {
    "doc": "FLAME",
    "title": "Example",
    "content": "import pandas as pd import dame_flame df = pd.read_csv(\"dame_flame/data/sample.csv\") model = dame_flame.matching.FLAME(repeats=False, verbose=1, early_stop_iterations=False) model.fit(holdout_data=df) result = model.predict(input_data=df) print(result) #&gt; x1 x2 x3 x4 #&gt; 0 0 1 1 * #&gt; 1 0 1 1 * #&gt; 2 1 0 * 1 #&gt; 3 1 0 * 1 . ",
    "url": "http://localhost:4000/documentation/api-documentation/FLAME#example",
    "relUrl": "/documentation/api-documentation/FLAME#example"
  },"10": {
    "doc": "FLAME",
    "title": "Methods",
    "content": "| init(self, adaptive_weights, alpha,…) | Initialize self | . | fit(self, holdout_data, treatment_col….) | Provide self with holdout data | . | predict(self, input_data…) | Perform the match on the input data | . init(adaptive_weights=’ridge’, alpha=0.1, repeats=True, verbose=2, early_stop_iterations=False, stop_unmatched_c=False, early_stop_un_c_frac=False, stop_unmatched_t=False, early_stop_un_t_frac=False, early_stop_pe=False, early_stop_pe_frac=0.01, early_stop_bf=False, early_stop_bf_frac=0.01, missing_indicator=np.nan, missing_data_replace=0, missing_holdout_replace=0, missing_holdout_imputations=10, missing_data_imputations=1, want_pe=False, want_bf=False) . Initialize self . fit(self, holdout_data=False, treatment_column_name=’treated’, outcome_column_name=’outcome’, weight_array=False)) . Provide self with holdout data . Parameters . treatment_column_name: string, optional (default=”treated”) This is the name of the column with a binary indicator for whether a row is a treatment or control unit. outcome_column_name: string, optional (default=”outcome”) This is the name of the column with the outcome variable of each unit. adaptive_weights: bool, “ridge”, “decision tree”, “ridgeCV”, optional (default=”ridge”) The method used to decide what covariate set should be dropped next. weight_array: array, optional If adaptive_weights = False, these are the weights to the covariates in input_data, for the non-adaptive version of FLAME. Must sum to 1. In this case, we do not use machine learning for the weights, they are manually entered as weight_array. predict(self, input_data) . Perform match and return matches . | Parameters: | input_data: dataframe-like (default=None). The dataframe on which to perform the matching | . | Returns: | Result: Pandas dataframe of matched units and covariates matched on | . ",
    "url": "http://localhost:4000/documentation/api-documentation/FLAME#methods",
    "relUrl": "/documentation/api-documentation/FLAME#methods"
  },"11": {
    "doc": "FLAME",
    "title": "FLAME",
    "content": " ",
    "url": "http://localhost:4000/documentation/api-documentation/FLAME",
    "relUrl": "/documentation/api-documentation/FLAME"
  },"12": {
    "doc": "API Documentation",
    "title": "API Documentation",
    "content": " ",
    "url": "http://localhost:4000/documentation/api-documentation/",
    "relUrl": "/documentation/api-documentation/"
  },"13": {
    "doc": "Post Processing",
    "title": "dame_flame.utils.post_processing",
    "content": "The treatment effect estimates and other utilities . import dame_flame.utils.post_processing # The main matched group of a unit or list of units MG(matching_object, unit_ids) # The conditional average treatment effect for a unit or list of units CATE(matching_object, unit_ids) # The average treatment effect for the matching data ATE(matching_object) # The average treatment effect on the treated for the matching data ATT(matching_object) . Parameters . matching_object: Class DAME or Class FLAME, required (no default). unit_id: int, list, required (no default): This is the unit or list of units for which the main matched group or treatment effect is being calculated . ",
    "url": "http://localhost:4000/documentation/api-documentation/post-processing#dame_flameutilspost_processing",
    "relUrl": "/documentation/api-documentation/post-processing#dame_flameutilspost_processing"
  },"14": {
    "doc": "Post Processing",
    "title": "Post Processing",
    "content": " ",
    "url": "http://localhost:4000/documentation/api-documentation/post-processing",
    "relUrl": "/documentation/api-documentation/post-processing"
  },"15": {
    "doc": "Documentation",
    "title": "Documentation",
    "content": " ",
    "url": "http://localhost:4000/documentation",
    "relUrl": "/documentation"
  },"16": {
    "doc": "Benefit to Early Stopping",
    "title": "Early Stopping and Treatment Effect Estimates",
    "content": "Both the FLAME and DAME algorithms begin by matching identical twins (“exact matches”) in the dataset. As iterations of the algorithm progress, later matched units are likely to have the highest error in estimated treatment effects. For this reason, there are situations where a user may wish to stop the FLAME or DAME algorithm in order to avoid poor quality matches, and if its not critical that all units are matched. From this example, we see that if high accuraccy between the estimated treatment effect and true treatment effect is a priority, then this algorithm should be stopped early. import numpy as np import pandas as pd import dame_flame import matplotlib.pyplot as plt from sklearn.metrics import mean_squared_error def draw_scatter(ax, x, y, title, color, mse, yticks= False): ax.scatter(x, y, c = color, alpha = 0.3, marker = 'o', edgecolor = 'black') ax.set_title(title, pad = 0.2, wrap = True, fontsize=labelsize*.75) ax.tick_params(labelsize=ticksize) ax.set_ylabel(\"Estimated CATT\", fontsize = labelsize*.75) ax.text(1, 35, \"MSE: {:.2f}\".format(mse), ha='center', va='center', fontsize=labelsize*.75) ax.set_xlabel('True CATT') # Generate Data df, true_catt = dame_flame.utils.data.gen_data_binx_decay_importance(num_control=1000, num_treated=1000, num_cov=10, bernoulli_param=0.5, bi_mean=2, bi_stdev=1) # Get Matches using the DAME algorithm model = dame_flame.matching.DAME(repeats=False, verbose=0) model.fit(holdout_data=df) model.predict(df) model_stop_early = dame_flame.matching.DAME(repeats=False, verbose=0, early_stop_un_c_frac=0.3) model_stop_early.fit(holdout_data=df) model_stop_early.predict(df) # Since not all units are matched, filter on those that are when finding CATT estimated_catt_full = [] true_catt_full = [] estimated_catt_early = [] true_catt_early = [] for unit in range(len(df)): if df.loc[unit]['treated'] == 1: temp_cate = dame_flame.utils.post_processing.CATE(model, unit) if temp_cate is not np.nan: estimated_catt_full.append(temp_cate) true_catt_full.append(true_catt[unit]) temp_cate = dame_flame.utils.post_processing.CATE(model_stop_early, unit) if temp_cate is not np.nan: estimated_catt_early.append(temp_cate) true_catt_early.append(true_catt[unit]) # Draw plot draw_scatter(axes[0], true_catt_early, estimated_catt_early, \"DAME, stopped at 30% control unmatched\", \"green\", mean_squared_error(true_catt_early, estimated_catt_early), True) draw_scatter(axes[1], true_catt_full, estimated_catt_full, \"DAME, matching all units\", \"green\", mean_squared_error(true_catt_full, estimated_catt_full), True) . Download Example From GitHub . ",
    "url": "http://localhost:4000/examples/early_stopping/#early-stopping-and-treatment-effect-estimates",
    "relUrl": "/examples/early_stopping/#early-stopping-and-treatment-effect-estimates"
  },"17": {
    "doc": "Benefit to Early Stopping",
    "title": "Benefit to Early Stopping",
    "content": " ",
    "url": "http://localhost:4000/examples/early_stopping/",
    "relUrl": "/examples/early_stopping/"
  },"18": {
    "doc": "Exact Matching Only",
    "title": "Exact Matching",
    "content": "The DAME or FLAME algorithm can be configured to be used for exact matching, using the early stopping criteria parameter early_stop_iterations=1. We illustrate this below. On a dataset with 200 units and five covariates that was generated with little randomness in which most covariate values are the same, we are only interested in exactly matched units. The output generated by the verbose=3 parameter shows that the majority of units are matched after one iteration. We visualize the treatment effect of each group, or the CATE of each group. We also visualize the matched groups’ sizes, which shows that one group has the majority of units. import dame_flame import matplotlib.pyplot as plt df,_ = dame_flame.utils.data.gen_data_decay_importance(num_control=100, num_treated=100, num_cov=5, bernoulli_param=0.9, bi_mean=2, bi_stdev=1) model = dame_flame.matching.FLAME(verbose=3, early_stop_iterations=1) model.fit(holdout_data=df) res = model.predict(df) groups = list(range(len(model.units_per_group))) cate_of_group = [] len_group = [] for group in model.units_per_group: cate_of_group.append(dame_flame.utils.post_processing.CATE(model, group[0])) len_group.append(len(group)) f, ax = plt.subplots(1, 2, gridspec_kw = {'width_ratios':[1, 1]}, figsize=(12,4)) ax[0].set_ylabel('Treatment Effect of Group', fontsize=14) ax[0].set_xlabel('Matched group ID number', fontsize=14) ax[0].set_title('Treatment Effect of Each Group of Perfectly Matched Units', fontsize=14) ax[0].bar(groups,cate_of_group) ax[1].set_ylabel('Number of Units in Group', fontsize=14) ax[1].set_xlabel('Matched group ID number', fontsize=14) ax[1].set_title('Size of Each Group of Perfectly Matched Units', fontsize=14) ax[1].bar(groups,len_group) plt.tight_layout() plt.savefig('treatment_effect_size_of_groups.png') . Download Example From GitHub . References . Matplotlib graphing . ",
    "url": "http://localhost:4000/examples/exact_matching/#exact-matching",
    "relUrl": "/examples/exact_matching/#exact-matching"
  },"19": {
    "doc": "Exact Matching Only",
    "title": "Exact Matching Only",
    "content": " ",
    "url": "http://localhost:4000/examples/exact_matching/",
    "relUrl": "/examples/exact_matching/"
  },"20": {
    "doc": "Examples",
    "title": "Examples",
    "content": " ",
    "url": "http://localhost:4000/examples",
    "relUrl": "/examples"
  },"21": {
    "doc": "Comparing DAME and FLAME",
    "title": "Comparing Match Quality of DAME and FLAME",
    "content": "Both the FLAME and DAME algorithms begin by matching any possible identical twins (“exact matches”) in the dataset, meaning any units that have the same values on every possible covariate. As the FLAME algorithm progresses to match units that do not have identical twins, each subsequent iteration of the FLAME algorithm will attempt to match on one fewer covariate. So, suppose the total number of covariates in a dataset is $r$. After any units that can be exact matched on $r$ have been found, the next iteration of FLAME will attempt to match on $r-1$ covariates. In the next iteration, it improves upon the previous covariate set used for matching, and match on $r-2$ covariates. However, DAME will consider any covariate set options that will yield the highest-quality matches. The size of covariates matched on does not necessarily need to decrease over iterations of the algorithm. This one of the key advantages the DAME algorithm has over FLAME. DAME produces higher quality matches, meaning that more units are matched on a large number of covariates. We show this below, running the same dataset on FLAME and DAME for 10 iterations. import numpy as np import pandas as pd import dame_flame import matplotlib.pyplot as plt # Generate Data num_covariates = 10 df, true_catt = dame_flame.utils.data.gen_data_binx_decay_importance(num_control=1000, num_treated=1000, num_cov=num_covariates, bernoulli_param=0.5, bi_mean=2, bi_stdev=1) # Get matches using DAME and FLAME model_dame = dame_flame.matching.DAME(repeats=False, verbose=0, early_stop_iterations=10) model_dame.fit(holdout_data=df) result_dame = model_dame.predict(df) model_flame = dame_flame.matching.FLAME(repeats=False, verbose=0, early_stop_iterations=10) model_flame.fit(holdout_data=df) result_flame = model_flame.predict(df) # pre-processing result_flame = result_flame.replace(to_replace='*', value=np.nan) result_dame = result_dame.replace(to_replace='*', value=np.nan) dict_matched_result_dame = {k:0 for k in range(0,num_covariates+1)} dict_matched_result_flame = {k:0 for k in range(0,num_covariates+1)} for i in result_flame.count(axis=1): dict_matched_result_flame[i] += 1 for i in result_dame.count(axis=1): dict_matched_result_dame[i] += 1 # plot x = np.arange(len(dict_matched_result_flame.keys())) # the label locations width = 0.5 # the width of the bars f, ax = plt.subplots(figsize=(12,9)) rects1 = ax.bar(x - width/2, dict_matched_result_flame.values(), width, color=\"green\", label = \"DAME\" ) #, stopping at {}% control units matched\".format(percent), hatch=\"/\") rects2 = ax.bar(x + width/2, dict_matched_result_dame.values(), width, color = \"orange\", label = \"FLAME\") #, stopping at {}% control units matched\".format(percent), hatch = \"\\\\\") ax.set_ylabel('Number of units', fontsize=16) ax.set_xlabel('Number of covariates matched on', fontsize=16) ax.set_title('Number of covariates that units were matched on after 10 iterations', fontsize=16) ax.set_xticks(x) ax.set_xticklabels(dict_matched_result_flame.keys()) ax.legend(fontsize=16) def autolabel(rects): \"\"\"Attach a text label above each bar in *rects*, displaying its height.\"\"\" for rect in rects: height = rect.get_height() ax.annotate('{}'.format(height), xy=(rect.get_x() + rect.get_width() / 2, height), xytext=(0, 3), # 3 points vertical offset textcoords=\"offset points\", ha='center', va='bottom') autolabel(rects1) autolabel(rects2) plt.show() . Download Example From GitHub . References . Matplotlib graphing . ",
    "url": "http://localhost:4000/examples/flame_vs_dame_quality/#comparing-match-quality-of-dame-and-flame",
    "relUrl": "/examples/flame_vs_dame_quality/#comparing-match-quality-of-dame-and-flame"
  },"22": {
    "doc": "Comparing DAME and FLAME",
    "title": "Comparing DAME and FLAME",
    "content": " ",
    "url": "http://localhost:4000/examples/flame_vs_dame_quality/",
    "relUrl": "/examples/flame_vs_dame_quality/"
  },"23": {
    "doc": "Interpreting Covariate Importance",
    "title": "Interpreting Covariate Importance",
    "content": "We say that this is an interpretable matching package because it will allow users to quickly and easily understand which covariates were selected to be important to their outcome. This can be useful in determining who benefits from treatment the most and where resources should be spent for future treatment. In this example, using the verbose==3 option, we show how to view the iterations of the algorithm and infer the best covariates. We begin with a simulated dataset in which five covariates are labelled 0 to 4, and the covariates are of exponentially decreasing importance to the outcome as the label numberincreases. We see from the output that the FLAME algorithm drops unimportant covariates earlier in its algorithm. A user who did not have knowledge of the data generating process could interpret from the output of FLAME that the covariates dropped were of lower importance than the covariates matched on. import dame_flame df,_ = dame_flame.utils.data.gen_data_decay_importance(num_control=50, num_treated=50, num_cov=5, bernoulli_param=0.5, bi_mean=2, bi_stdev=1) model = dame_flame.matching.FLAME(verbose=3, want_pe=True, want_bf=True) model.fit(holdout_data=df) res = model.predict(df) . Download Example From GitHub . ",
    "url": "http://localhost:4000/examples/interpretability/",
    "relUrl": "/examples/interpretability/"
  },"24": {
    "doc": "Algorithm Controls",
    "title": "Early Stopping Controls",
    "content": ". | Introduction to Early Stopping Controls | Recommendations | . This goes in the Algorithm Controls page. ",
    "url": "http://localhost:4000/documentation/user-guide/Algorithm-Controls#early-stopping-controls",
    "relUrl": "/documentation/user-guide/Algorithm-Controls#early-stopping-controls"
  },"25": {
    "doc": "Algorithm Controls",
    "title": "Introduction to Early Stopping Controls",
    "content": "The ideal situation for matching in causal inference is if each treatment unit has an exactly identical control unit. We can best determine the rise in income that a person experiences after a job training program if that person has an identical twin with the same degree and GPA as them who didn’t attend the job training program. The FLAME-DAME package begins by matching identical twins (“exact matches”) in the dataset. Since not all units have exact matches, most units are matched based on subsets of all covariates. The subset that a unit is matched on is the subsets that is selected to be most predictive of their outcome. As the FLAME and DAME algorithms run, the units that are matched later in the algorithm, are those that are most distinct in observable characteristics from the other units in the dataset are matched later. Later matched units are likely to have the highest error in estimated treatment effects. For this reason, there are situations where the FLAME or DAME algorithm should be stopped early in order to avoid poor matches. ",
    "url": "http://localhost:4000/documentation/user-guide/Algorithm-Controls#introduction-to-early-stopping-controls",
    "relUrl": "/documentation/user-guide/Algorithm-Controls#introduction-to-early-stopping-controls"
  },"26": {
    "doc": "Algorithm Controls",
    "title": "Recommendations",
    "content": "The default option is that the algorithm runs until all units are matched. However, if runtime or high accuracy of estimates of treatment effects are important, then we recommend users experiment with their stopping criteria based on their specific need and dataset size. A large dataset will have a longer runtime, and an early stop will take less time. Regardless of the early stopping criteria chosen, in the majority of datasets, any early stopping will lead to closer estimates between the estimated and true treatment effects. This is illustrated in the examples section. If it is crucial that all units be matched, it is recommended that users do not use any early stopping criteria. | Category of Early Stopping | Technical Details | Usage Recommendation | Algorithm parameters | . | Algorithm Iterations | This provides a number of iterations after which to stop the DAME or FLAME algorithm. If FLAME is used, then this is the maximum number of covariates that can be dropped, meaning when the total number of covariates is $m$, no unit will be matched on $m-$early_stop_iterations covariates | This is useful in the case of a FLAME user knowing their preferred covariate match size, or if a user knows what runtime is sufficient from a previous experiment | early_stop_iterations | . | Unmatched Units in Treatment or Control | When the algorithm is set with the repeats=True parameter, then previously matched units can still be placed in groups with other units. The algorithm will by default stop iterating when there are no more units that have not been placed in any group. However, a case could arise where all units remaining to be placed in a group are of the treatment or control group, and we provide this option in case a user has preference between ensuring that all treated or control units are matched. | These parameters will not be useful, and is therefore not recommended in the case where the the repeats parameter is False. If repeats=False, then in effect, both of these parameters are True. | stop_unmatched_c, stop_unmatched_t | . | Proportion of unmatched units | This stops the algorithm when some fraction of control units or treatment units are unmatched | One specific case in which this could be useful immediately is where a user is certain that some percent of the input is unlikely to result in good matches. | early_stop_un_c_frac, early_stop_un_t_frac | . | Predictive Error | The predictive error measures how important a covariate set is for predicting the outcome on the holdout dataset, using a machine learning algorithm. It is the sole determinant of the covariate set to match on for DAME, and one of two factors for FLAME. | The range of this value is specific to a dataset’s values. Therefore, reasonable values for this can only be determined after at least one prior run of this algorithm on the same dataset in which the predictive error is observed. | early_stop_pe, early_stop_pe_frac | . | Balancing Factor | The balancing factor of an iteration is the number of matches formed after selecting a covariate set, and the discrepancy between the number of treated and control units remaining to be matched after the matching. This is only part of the algorithm’s decision of which covariates to drop for FLAME but is still measured for DAME. | If it’s important to a user that there be a balance between treatment and control units in each covariate set, this is a parameter to pay attention to. The range of this value is specific to a dataset’s values. Therefore, reasonable values for this can only be determined after at least one prior run of this algorithm on the same dataset, while observing the balancing factor. | early_stop_bf, early_stop_bf_frac | . ",
    "url": "http://localhost:4000/documentation/user-guide/Algorithm-Controls#recommendations",
    "relUrl": "/documentation/user-guide/Algorithm-Controls#recommendations"
  },"27": {
    "doc": "Algorithm Controls",
    "title": "Algorithm Controls",
    "content": " ",
    "url": "http://localhost:4000/documentation/user-guide/Algorithm-Controls",
    "relUrl": "/documentation/user-guide/Algorithm-Controls"
  },"28": {
    "doc": "Getting Matches",
    "title": "Getting Matches from the Data",
    "content": ". | FLAME . | References | . | DAME . | References | . | Variations in the learning of the best covariate set . | References | . | . ",
    "url": "http://localhost:4000/documentation/user-guide/Getting-Matches#getting-matches-from-the-data",
    "relUrl": "/documentation/user-guide/Getting-Matches#getting-matches-from-the-data"
  },"29": {
    "doc": "Getting Matches",
    "title": "FLAME",
    "content": "FLAME stands for Fast Large Scale Almost Matching Exactly. The FLAME algorithm begins by matching any units that can be matched exactly on all covariates. The algorithm will iterate over all covariates until stopping criteria is reached. In each iteration, the algorithm will drop the worst covariate set to match on, and units that have identical values in all of the remaining covariates will form a matched group. When deciding which covariate should be dropped, at each step, it drops the covariate leading to the smallest drop in match quality, MQ, defined as MQ=C·BF−PE. Here, PE denotes the predictive error, which measures how important the dropped covariate is for predicting the outcome on the holdout dataset, using a machine learning algorithm. The balancing factor, BF, measures the number of matches formed by dropping that covariate and the discrepancy between the number of treated and control units after the matching. In future iterations, the covariate that was determined worst and was just dropped will not reappear, so the maximum number of times the algorithm will iterate is equal to the number of covariates. import pandas as pd import dame_flame df = pd.read_csv(\"dame_flame/data/sample.csv\") model = dame_flame.matching.FLAME(repeats=False, verbose=1, early_stop_iterations=False) model.fit(holdout_data=df) result = model.predict(input_data=df) print(result) #&gt; x1 x2 x3 x4 #&gt; 0 0 1 1 * #&gt; 1 0 1 1 * #&gt; 2 1 0 * 1 #&gt; 3 1 0 * 1 . References . For more details on this algorithm, see Wang, Tianyu, et al. “Flame: A fast large-scale almost matching exactly approach to causal inference.” . ",
    "url": "http://localhost:4000/documentation/user-guide/Getting-Matches#flame",
    "relUrl": "/documentation/user-guide/Getting-Matches#flame"
  },"30": {
    "doc": "Getting Matches",
    "title": "DAME",
    "content": "DAME stands for Dynamic Almost Matching Exactly. The algorithm begins by matching any units that can be matched exactly on all co-variates. The algorithm will iterate over options of covariates to match on until stopping criteria is reached. In each iteration, the algorithm will select the best covariate set to match on, and units that have identical values in all of the covariates that are part of the chosen covariate set will form a matched group. In its options of covariate sets to drop, DAME will always include the largest possible covariate sets, and will ultimately consider several combinations of covariates before selecting one to match on. It defines the best covariate set as the one that minimizes PE. PE is predictive error, and measures how important the covariate set is for predicting the outcome on the holdout dataset, using a machine learning algorithm . import pandas as pd import dame_flame df = pd.read_csv(\"dame_flame/data/sample.csv\") model = dame_flame.matching.DAME(repeats=False, verbose=1, early_stop_iterations=False) model.fit(holdout_data=df) result = model.predict(input_data=df) print(result) #&gt; x1 x2 x3 x4 #&gt; 0 0 1 1 * #&gt; 1 0 1 1 * #&gt; 2 1 0 * 1 #&gt; 3 1 0 * 1 . References . For more details on this algorithm, see Dieng, Awa, et al. “Interpretable almost-exact matching for causal inference.” . ",
    "url": "http://localhost:4000/documentation/user-guide/Getting-Matches#dame",
    "relUrl": "/documentation/user-guide/Getting-Matches#dame"
  },"31": {
    "doc": "Getting Matches",
    "title": "Variations in the learning of the best covariate set",
    "content": "Both the FLAME and DAME algorithms choose the best covariate set after measuring how important each covariate set is for predicting the outcome on the holdout dataset, using a machine learning algorithm. We offer different options for the machine learning algorithm used, as well as a simplified FLAME and simplified DAME that does not use machine learning. | Learning Method | Technical Details | Usage Recommendation | Algorithm parameter | . | Ridge Regression | A ridge regression is similar to an ordinary least squares regression, but it imposes a penalty on the size of coefficients. It minimizes a residual sum of squares. A shrinkage parameter, $\\alpha$ must be included. | This can only be used when it is certain that none of the covariates are categorical. Ordinal, binary, and discrete numerical data is all accepted. For this option, a larger $\\alpha$ corresponds should be chosen if it is believed that there is greater multicollinearity in the data, meaning that many covariates are linearly correlated. | adaptive_weights='ridge' | . | Ridge Regression CV | This is a ridge regression with built-in cross validation to determine the best $\\alpha$ parameter. We use the scikit-learn ridgeCV class, but the default array of $\\alpha$ options that we provide the function to iterate over is larger than the default they provide, for greater flexibility. | This also can only be used when it is certain that none of the covariates are categorical. Ordinal, binary and discrete numerical data is all accepted. This option is advantageous over the ‘ridge’ option without cross validation in a case where a user is uncertain about the $\\alpha$ parameter, and a minor speed decrease from cross validation is acceptable. | adaptive_weights='ridgeCV' | . | Decision Tree | The underlying implementation is the Decision Tree Regression provided by scikit-learn, which uses a variation of CART. Trees predict the value of the outcome by learning decision rules from the covariates. | This can be used for categorical, ordinal, binary, and discrete numerical data. Overfitting is a risk with decision tree models, which can be possible in DAME or FLAME algorithm if the holdout and input datasets provided are the same. | adaptive_weights='decision-tree' | . References . We use scikit-learn for the underlying learning algorithms. So we refer you to their documentation and references to learn more about these popular machine learning algorithms, as well as their specific implementations: . Scikit-learn Ridge Regressions. Scikit-learn RidgeCV. Scikit-learn DecisionTree. For examples of categorical, binary, and numerical data, see here. ",
    "url": "http://localhost:4000/documentation/user-guide/Getting-Matches#variations-in-the-learning-of-the-best-covariate-set",
    "relUrl": "/documentation/user-guide/Getting-Matches#variations-in-the-learning-of-the-best-covariate-set"
  },"32": {
    "doc": "Getting Matches",
    "title": "Getting Matches",
    "content": " ",
    "url": "http://localhost:4000/documentation/user-guide/Getting-Matches",
    "relUrl": "/documentation/user-guide/Getting-Matches"
  },"33": {
    "doc": "Introduction to Causal Inference",
    "title": "Introduction",
    "content": ". | Introduction to Causal Inference | Introduction to Matching . | Further Reference | . | . ",
    "url": "http://localhost:4000/documentation/user-guide/Introduction#introduction",
    "relUrl": "/documentation/user-guide/Introduction#introduction"
  },"34": {
    "doc": "Introduction to Causal Inference",
    "title": "Introduction to Causal Inference",
    "content": "Causal inference is the attempt to draw conclusions that something is being caused by something else. It goes beyond questions of correlation, association, and is distinct from model-based predictive analysis. Questions of robust causal inference are practically unavoidable in health, medicine, or social studies. Much of the available data in the clinical and social sciences is observational, and we can only observe one outcome per individual. For example, if one individual took pain reliever for a headache and they now feel better, we don’t know what would have happened to that same individual over the same time period, if they had not taken pain reliever. Taking the pain reliever puts them in the treatment group, but since we don’t know what the control outcome of not taking pain reliever would be (without time travel), how can we say pain reliever caused the headache to go away? . ",
    "url": "http://localhost:4000/documentation/user-guide/Introduction",
    "relUrl": "/documentation/user-guide/Introduction"
  },"35": {
    "doc": "Introduction to Causal Inference",
    "title": "Introduction to Matching",
    "content": "When estimating causal effects in an observational setting, one common approach is to match each treatment unit to an identical control unit. Going back to the example, can we find two people sharing every physical attribute, who also had the exact same symptoms, prior to the time when only one of them taking the pain reliever? Secondly, how did their outcomes differ? . In large datasets where we observe many characteristics about individuals, few “identical twins”, (referred to as “exact matches”) exist. What is the best way to match individuals that were treated and controlled? Only once they’re matched are we able to apply common treatment effect estimators to the groups of matched individuals, in order to try to determine the effect of treatment. Further Reference . For further reference on causal inference research and its assumptions and issues, we recommend Imbens, Guido W., and Donald B. Rubin. Causal inference in statistics, social, and biomedical sciences.. ",
    "url": "http://localhost:4000/documentation/user-guide/Introduction#introduction-to-matching",
    "relUrl": "/documentation/user-guide/Introduction#introduction-to-matching"
  },"36": {
    "doc": "Missing Data Handling",
    "title": "Missing Data Handling",
    "content": ". | Introduction to Missing Data Problems in Matching | Determining Which Missing Data Method Is Right For You . | Missing values in the input data | Missing values in the holdout data | . | Further Details on MICE imputation . | Further Reference | . | . ",
    "url": "http://localhost:4000/documentation/user-guide/Missing-Data",
    "relUrl": "/documentation/user-guide/Missing-Data"
  },"37": {
    "doc": "Missing Data Handling",
    "title": "Introduction to Missing Data Problems in Matching",
    "content": "Missing data is a complicated issue in matching problems. Imputing missing values on datasets is possible, but matches become less interpretable when matching on imputed values, in that it is more difficult to discern why a match was recommended by the matching algorithm. The DAME and FLAME algorithms rely on covariate matching, so the DAME-FLAME package is able to take advantage of this and allows users to match on raw values on data sets with missing data without imputing any data. The DAME-FLAME package also provides options for imputing data. ",
    "url": "http://localhost:4000/documentation/user-guide/Missing-Data#introduction-to-missing-data-problems-in-matching",
    "relUrl": "/documentation/user-guide/Missing-Data#introduction-to-missing-data-problems-in-matching"
  },"38": {
    "doc": "Missing Data Handling",
    "title": "Determining Which Missing Data Method Is Right For You",
    "content": "Missing values in the input data . We recommend users set the parameter missing_data_replace=2, where units that have missing values are still matched on, but the covariates they are missing are not used in computing their match. In this option, the underlying algorithm works by replacing each missing value with a unique value, so that in the matching procedure, those covariates simply don’t have a match because their values are not equl to any other values. It is not recommended to use MICE to impute on the matching dataset, as this would be very slow. Users also have the option of imputing their data through any data imputation method of their choice, and then using their imputed dataset as the input data. | Method | Recommendation | Technical Details | missing_data_replace parameter value | . | Do not match units with missing values | Only use if missing values indicate bad unit | Units in the input dataset that have missing data are dropped from the dataset prior to running the algorithms finding the matches | 1 | . | Match units with missing values, but ignore missing values | Recommended for most cases | When pre-processing the input, we place a unique value in place of each missing data point. This will not match any other value, so a unit will only be matched where it’s non-missing covariates match the non-missing covariates of another unit | 2 | . | Impute missing values with MICE | Not recommended | Creates several imputed datasets and iterates over each to find a match according to each dataset. See below for details. | 3 | . Missing values in the holdout data . The “holdout dataset”, if provided, must have the exact same covariates as the input dataset. It is used when training and fitting a machine learning algorithm to determine the best covariates for predicting the outcome. Matches will always be done on the corresponding covariates, but only on the input dataset. We recommend users set the parameter missing_data_replace=1, where units with missing values are dropped, and these units are not used in determining the best covariate set for predicting the outcome. This is the fastest option for the algorithm’s runtime. The error of the predictions will vary depending on how large and informative the observed, non-missing dataset is. Users also have the option of imputing their data through any data imputation method of their choice, and then using their imputed dataset as the input data. | Method | Recommendation | Technical Details | missing_holdout_replace parameter value | . | Do not match units with missing values | Recommended | Units in the holdout dataset that have missing data are dropped from the dataset prior to running the algorithms finding the matches | 1 | . | Impute missing values with MICE | Recommended if interpretability and speed are lower order priorities | Creates several imputed holdout datasets. When choosing the best covariate set for predicting the outcome, iterates over each imputed dataset, and averages the predictive error over all datasets | 2 | . ",
    "url": "http://localhost:4000/documentation/user-guide/Missing-Data#determining-which-missing-data-method-is-right-for-you",
    "relUrl": "/documentation/user-guide/Missing-Data#determining-which-missing-data-method-is-right-for-you"
  },"39": {
    "doc": "Missing Data Handling",
    "title": "Further Details on MICE imputation",
    "content": "The built-in imputation method that we include is the “Multiple Imputation by Chained Equations” algorithm. This constructs several imputed datasets. It fills in missing values multiple times, creating multiple “complete” datasets. The error of the imputations, and the consistency of the imputations across imputed datasets, is dependent on how predictive the observed data is of the missing values. The underlying MICE implementation is done using scikit learn’s experimental IterativeImpute package, and relies on DecisionTreeRegressions in the imputation process, to ensure that the data generated is fit for unordered categorical data. Further Reference . For further reference on the MICE missing data handling technique, we recommend Azur, Melissa J., et al. “Multiple imputation by chained equations: what is it and how does it work?.”. ",
    "url": "http://localhost:4000/documentation/user-guide/Missing-Data#further-details-on-mice-imputation",
    "relUrl": "/documentation/user-guide/Missing-Data#further-details-on-mice-imputation"
  },"40": {
    "doc": "To Match or Not",
    "title": "To Match or Not",
    "content": "That is the question . | To Match or Not . | Determining Whether to Use Matching Methods . | The Stable Unit Treatment Value Assumption (SUTVA) | The Unconfoundedness Assumption | Overlap of Treatment and Control Groups | Additional Requirements For DAME-FLAME | . | Challenges in Matching Methods . | Further Reference | . | . | . ",
    "url": "http://localhost:4000/documentation/user-guide/to-match-or-not",
    "relUrl": "/documentation/user-guide/to-match-or-not"
  },"41": {
    "doc": "To Match or Not",
    "title": "Determining Whether to Use Matching Methods",
    "content": "Matching of treatment and control units can be a good method in order to determine treatment effects. However, certain criteria must be upheld in order for matching to be an appropriate solution for a given dataset. If these criteria are not upheld, perhaps other approaches to causal inference should be used in place of, or in addition to matching. The Stable Unit Treatment Value Assumption (SUTVA) . Treatments applied to one unit should not affect the outcome of another unit. Units can not interfere with one another. This is reasonable in many situations: If two individuals are not in contact with each other, how would one individual taking a pain medication impact the outcome of another individual. We should also assume that the treatment doesn’t have varying forms, and is completely binary. Individuals can not have taken pain medication of different strengths. The Unconfoundedness Assumption . This is also referred to as “ignorability”. It is important that the outcome is independent of the treatment when observable covaraiates are held constant. Omitted variable bias is a common issue that occurs when a variable impacts both treatment and outcomes, and appears in a bias of treatment effect estimates. In the example about pain medications, if a researcher fails to include in their dataset some underlying health condition that impacts response to pain medication, the impact of taking pain medication for a headache might be evaluated incorrectly. Overlap of Treatment and Control Groups . A common problem in causal inference is overlap or imbalance between treatment and control groups. A treatment and control group would have no overlap if none of the covariates have the same values. In this case, the FLAME and DAME algorithms would not find any matches, and no treatment effect estimates would be possible. A more moderate issue is partial overlap. In this case, some units do not have matches. Because the DAME-FLAME package allows for algorithm controls, even if all units could be matched in theory, users of the algorithm might prefer to avoid matching all units. Regardless of the cause, units that are unmatched do not have a CATE estimate, and they are not included in the treatment effect calculations either. Additional Requirements For DAME-FLAME . As a final note, DAME-FLAME is intended for use on datasets that contain discrete covariates. We do not recommend users bin covariate datasets unless they are confident they are binning a separable dataset in a way that is typical of their research question. ",
    "url": "http://localhost:4000/documentation/user-guide/to-match-or-not#determining-whether-to-use-matching-methods",
    "relUrl": "/documentation/user-guide/to-match-or-not#determining-whether-to-use-matching-methods"
  },"42": {
    "doc": "To Match or Not",
    "title": "Challenges in Matching Methods",
    "content": "“Exact matching” isn’t possible when we a dataset has lots of characteristics about individuals, or is high dimensional. So, matching methods performing the best-possible alternative should be interpretable. Users of matching algorithms need to be able to easily understand which covariates were selected to be most important to their outcome, and need be able to find out why they were selected. This is important so that causal analysis can provide crucial information on who benefits from treatment most, where resources should be spent for future treatments, and why some individuals benefit from treatment while others were not. This can also help researchers determine what type of additional data must be collected. Secondly, the matches should also be high quality. If an oracle could tell us the exact result of doing treatment on any individual whose treatment we did not observe, then would we find that our estimate of the effect of treatment on that individual is accurate? . Further Reference . For further reference on causal inference research and its assumptions and issues, we recommend Imbens, Guido W., and Donald B. Rubin. Causal inference in statistics, social, and biomedical sciences.. ",
    "url": "http://localhost:4000/documentation/user-guide/to-match-or-not#challenges-in-matching-methods",
    "relUrl": "/documentation/user-guide/to-match-or-not#challenges-in-matching-methods"
  },"43": {
    "doc": "Treatment Effect Estimates",
    "title": "Treatment Effect Estimates",
    "content": "We define and discuss the most common metrics that researchers estimate when evaluating the results of a treatment. | Notes on Statistical Assumptions . | Unconfoundedness | Overlap of Treatment and Control Groups | . | Standard Notation for Statistical Definitions | Conditional Average Treatment Effect (CATE) | Average Treatment Effect (ATE) . | Further Reference | . | . ",
    "url": "http://localhost:4000/documentation/user-guide/Treatment-Effects",
    "relUrl": "/documentation/user-guide/Treatment-Effects"
  },"44": {
    "doc": "Treatment Effect Estimates",
    "title": "Notes on Statistical Assumptions",
    "content": "Unconfoundedness . As an important note, these metrics will be biased if the unconfoundedness or ignorability assumption does not hold. In other words, it is best to use these metrics if the outcome is independent of the treatment when observable covaraiates are held constant. The bias will be as small as possible if many covariates are included, or users include any covariate that would have an impact on the outcome. In a case where a user is conflicted on whether or not to include a particular covariates, we recommend users consider including all variables and performing a match, while observing how quickly in the algorithm that covariate is dropped with the verbose=3 parameter. If the covariate is dropped in an early iteration with a low predictive error, it can be assumed that covariate is only weakly correlated with the outcome. Users can then perform a match a second time after removing weakly correlated covariates from their dataset if they wish. We recommend this approach, in order to ensure that any important covariates are matched on and bias of these estimators is minimized. Overlap of Treatment and Control Groups . A common problem in causal inference is overlap or imbalance between treatment and control groups. A treatment and control group would have no overlap if none of the covariates have the same values. In this case, the FLAME and DAME algorithms would not find any matches, and no treatment effect estimates would be possible. A more moderate issue is partial overlap. In this case, some units do not have matches. Because the DAME-FLAME package allows for algorithm controls, even if all units could be matched in theory, users of the algorithm might prefer to avoid matching all units. Regardless of the cause, units that are unmatched do not have a CATE estimate, and they are not included in the ATE calculations either. ",
    "url": "http://localhost:4000/documentation/user-guide/Treatment-Effects#notes-on-statistical-assumptions",
    "relUrl": "/documentation/user-guide/Treatment-Effects#notes-on-statistical-assumptions"
  },"45": {
    "doc": "Treatment Effect Estimates",
    "title": "Standard Notation for Statistical Definitions",
    "content": "We refer to each matched unit as unit $i$. There are there are $N$ matched unit in total. We may interchangeably refer to the matched units as ‘individuals’ or ‘observations’, and although we will not always preface by saying they are ‘matched units’, please remember that they must be in order to be included in treatment effects. There are $r$ covariates upon which we have observed characteristics about each of the individuals prior to treatment, and these are $x_1$ through $x_r$. For a given unit $i$, its vector of covariates is $X_i$ . Let the treatment indicator for any unit $i$ be indicated as $T_i$. We let $Y_i$ be the outcome for individual $i$. We use this interchangably with the notation $Y_i(T_i)$, so we write $Y_i(0)$ to indicate the outcome of $i$ if $i$ is in the control group, and $Y_i(1)$ if $i$ is in the treated group. Lastly, we introduce notation for the matched grous. We label a matched group as $m$. The size of a matched group is $| m\\vert$, and this is the number of units in the group. There are $M$ matched groups in total. ",
    "url": "http://localhost:4000/documentation/user-guide/Treatment-Effects#standard-notation-for-statistical-definitions",
    "relUrl": "/documentation/user-guide/Treatment-Effects#standard-notation-for-statistical-definitions"
  },"46": {
    "doc": "Treatment Effect Estimates",
    "title": "Conditional Average Treatment Effect (CATE)",
    "content": "This is defined as the average treatment effect conditional on particular covariates. We provide an implementation of CATE that allows a user to input a unit $i$, and then will output the CATE based on the covariates that $i$ was matched on. Formally, CATE for covariates $X_i$ is $\\frac{1}{N}\\sum_{i=1}^N\\mathbb{E}[Y(1)-Y(0)|X_i]$ . Since our units are each matched in a group with other units that share treatment indicator as well as the opposite indicator, each unit in a matched group will have the same CATE. For a unit $i$ in matched group $M$ of size $||M||$, we estimate the CATE of $i$ as: \\(\\frac{1}{\\|M\\|}\\sum_{i:T_i=1}[\\hat{Y}_i(1)]-\\frac{1}{\\|M\\|}\\sum_{i:T_i=0}[\\hat{Y}_i(0)]\\) . ",
    "url": "http://localhost:4000/documentation/user-guide/Treatment-Effects#conditional-average-treatment-effect-cate",
    "relUrl": "/documentation/user-guide/Treatment-Effects#conditional-average-treatment-effect-cate"
  },"47": {
    "doc": "Treatment Effect Estimates",
    "title": "Average Treatment Effect (ATE)",
    "content": "The Average Treatment Effect for a population is generally $\\mathbb{E}[Y(1)-Y(0)]$. We estimate this in a way that is robust to units reappearing in matched groups, according to the parameter repeats=True. Our estimate will ensure that units appearing in multiple matched groups are weighted accordingly, as well as that their influence in their groups is accounted for. Let $q_i$ denote the number of matched groups that unit $i$ appears in. Then, for a given matched group $m$, the number of matches made by the units in $m$ is $w_m=\\sum_{i=1}^{|m|}\\frac{1}{q_i}$. Since the CATE of each unit in a group is the same, we can call the CATE of group $m$ $\\mathit{CATE}_m$. So, finally, we estimate ATE as $\\frac{\\sum_{m\\in\\mathbb{M}}CATE_m*w_m}{\\sum_{m\\in\\mathbb{M}}w_m}$ . Further Reference . For further reference on treatment effects, see Imbens, Guido W. “Nonparametric estimation of average treatment effects under exogeneity: A review.” . ",
    "url": "http://localhost:4000/documentation/user-guide/Treatment-Effects#average-treatment-effect-ate",
    "relUrl": "/documentation/user-guide/Treatment-Effects#average-treatment-effect-ate"
  },"48": {
    "doc": "User Guide",
    "title": "User Guide",
    "content": " ",
    "url": "http://localhost:4000/documentation/user-guide/",
    "relUrl": "/documentation/user-guide/"
  },"49": {
    "doc": "Getting Started",
    "title": "Getting Started",
    "content": "Here, we aim to get you launched . | Dependencies | Installation | Quickstart Example | . ",
    "url": "http://localhost:4000/getting-started",
    "relUrl": "/getting-started"
  },"50": {
    "doc": "Getting Started",
    "title": "Dependencies",
    "content": "This package requires prior installation of . | Python (&gt;= 3.0) | NumPy (&gt;= 1.17.5) | Scikit-Learn (&gt;= 0.22.1)) | Pandas (todo: check) | . If your computer system does not have python 3.*, install from here. If your python version does not have the Pandas, Scikit learn, or Numpy packages, install from here . ",
    "url": "http://localhost:4000/getting-started#dependencies",
    "relUrl": "/getting-started#dependencies"
  },"51": {
    "doc": "Getting Started",
    "title": "Installation",
    "content": "The DAME-FLAME Python Package is available for download on the almost-matching-exactly Github or via PyPi (recommended): . pip install dame-flame . ",
    "url": "http://localhost:4000/getting-started#installation",
    "relUrl": "/getting-started#installation"
  },"52": {
    "doc": "Getting Started",
    "title": "Quickstart Example",
    "content": "We run the DAME function with the following basic command. In this example, we provide only the basic inputs: (1) input data as a dataframe or file, (2) the name of the outcome column, and (3) the name of the treatment column. In this example, because of the toy sized small dataset, we set the holdout dataset equal to the complete input dataset. import pandas as pd import dame_flame df = pd.read_csv(\"dame_flame/data/sample.csv\") model = dame_flame.matching.DAME(repeats=False, verbose=1, early_stop_iterations=False) model.fit(holdout_data=df) result = model.predict(input_data=df) print(result) #&gt; x1 x2 x3 x4 #&gt; 0 0 1 1 * #&gt; 1 0 1 1 * #&gt; 2 1 0 * 1 #&gt; 3 1 0 * 1 print(model.groups_per_unit) #&gt; 0 1.0 #&gt; 1 1.0 #&gt; 2 1.0 #&gt; 3 1.0 print(model.units_per_group) #&gt; [[2, 3], [0, 1]] . result is type Data Frame. The dataframe contains all of the units that were matched, and the covariates and corresponding values, that it was matched on. The covariates that each unit was not matched on is denoted with a “ * “ character. model.groups_per_unit is a Data Frame with a column of unit weights which specifies the number of groups that each unit was placed in. model.units_per_group is a list in which each list is a main matched group, and the unit ids that belong to that group. Additional values based on additional optional parameters can be retrieved, detailed in additional documentation below. To find the main matched group of a particular unit or group of units after DAME has been run, use the function MG: . mmg = dame_flame.utils.post_processing.MG(matching_object=model, unit_id=0) print(mmg) #&gt; x1 x2 x3 x4 treated outcome #&gt; 0 0 1 1 * 0 5 #&gt; 1 0 1 1 * 1 6 . To find the conditional treatment effect (CATE) for the main matched group of a particular unit or group of units, use the function CATE: . te = dame_flame.utils.post_processing.CATE(matching_object=model, unit_id=0) print(te) #&gt; 3.0 . To find the average treatment effect (ATE) or average treatment effect on the treated (ATT), use the functions ATE and ATT, respectively: . ate = dame_flame.utils.post_processing.ATE(matching_object=model) print(ate) #&gt; 2.0 att = dame_flame.utils.post_processing.MG(matching_object=model) print(att) #&gt; 2.0 . ",
    "url": "http://localhost:4000/getting-started#quickstart-example",
    "relUrl": "/getting-started#quickstart-example"
  },"53": {
    "doc": "Home",
    "title": "Welcome to the DAME-FLAME Python Package Documentation!",
    "content": "dame-flame is a Python package for performing matching for observational causal inference on datasets containing discrete covariates. It implements the Dynamic Almost Matching Exactly (DAME) and Fast, Large-Scale Almost Matching Exactly (FLAME) algorithms, which match treatment and control units on subsets of the covariates. The resulting matched groups are interpretable, because the matches are made on covariates (rather than, for instance, propensity scores), and high-quality, because machine learning is used to determine which covariates are important to match on. View us on GitHub . ",
    "url": "http://localhost:4000/#welcome-to-the-dame-flame-python-package-documentation",
    "relUrl": "/#welcome-to-the-dame-flame-python-package-documentation"
  },"54": {
    "doc": "Home",
    "title": "Contact",
    "content": "Please reach out to let our team know if you’re using this, or if you have any questions! Contact Neha Gupta at neha.r.gupta “at” duke “dot” edu . ",
    "url": "http://localhost:4000/#contact",
    "relUrl": "/#contact"
  },"55": {
    "doc": "Home",
    "title": "Home",
    "content": " ",
    "url": "http://localhost:4000/",
    "relUrl": "/"
  }
}
